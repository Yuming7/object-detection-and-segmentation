{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g-PCLCl6ZbBt","outputId":"ea33cf92-0194-4c3e-e99d-31237673d298","executionInfo":{"status":"ok","timestamp":1699506808531,"user_tz":420,"elapsed":1478,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# Google Colab Patch\n","use_colab = True\n","if use_colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import sys\n","    # ----------------------------------------\n","    dir = \"/content/drive/MyDrive/Colab Notebooks/328/assignment4\"\n","    # ----------------------------------------\n","    sys.path.append(dir)\n","from A4_utils import *"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"x3N7Kgbhe-Gt","executionInfo":{"status":"ok","timestamp":1699506828704,"user_tz":420,"elapsed":14709,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}}},"outputs":[],"source":["import torch\n","from torch.utils.data.dataset import Dataset  # For custom data-sets\n","from torchvision import transforms\n","import torchvision\n","from skimage.io import imread\n","from PIL import Image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from tqdm import tqdm\n","\n","CLASSES = [\n","    '__background__', '0'\n","    '1', '2', '3',\n","    '4', '5', '6',\n","    '7', '8', '9'\n","]\n","\n","Num_classes = 11\n","\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/328/assignment4/mnistdd_rgb_train_valid/\"\n","\n","# training\n","train_image_path = data_path + 'train_X.npy'\n","train_label_path = data_path + 'train_Y.npy'\n","train_bboxes_path = data_path + 'train_bboxes.npy'\n","train_seg_path = data_path + 'train_seg.npy'\n","\n","# validation\n","valid_image_path= data_path + 'valid_X.npy'\n","valid_label_path= data_path +'valid_Y.npy'\n","valid_bboxes_path = data_path +'valid_bboxes.npy'\n","valid_seg_path = data_path +'valid_seg.npy'\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, label_paths, bounding_box_path, train=True):   # initial logic happens like transform\n","        self.images = np.load(image_paths)\n","        self.label =  np.load(label_paths)\n","        self.bboxes = np.load(bounding_box_path)\n","        self.transforms_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n","                                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","    def __getitem__(self, index):\n","\n","         # Retrieve and preprocess a sample\n","        image = self.images[index].reshape(64, 64, 3)\n","        labels = torch.from_numpy(self.label[index]).to(torch.int64)\n","\n","        bounding_boxes1 = self.bboxes[index][0]\n","        bounding_boxes2 = self.bboxes[index][1]\n","\n","        boxes = torch.zeros([2,4], dtype=torch.float32)\n","        boxes1 = [bounding_boxes1[1], bounding_boxes1[0], bounding_boxes1[3], bounding_boxes1[2]]\n","        boxes2 = [bounding_boxes2[1], bounding_boxes2[0], bounding_boxes2[3], bounding_boxes2[2]]\n","        boxes[0] = torch.tensor(boxes1)\n","        boxes[1] = torch.tensor(boxes2)\n","\n","        # Apply any transformations if needed\n","        image = self.transforms_image(image)\n","\n","        return {\"image\":image, 'boxes': boxes, 'labels': labels}\n","\n","    def __len__(self):  # return count of sample we have\n","        return len(self.images)\n","\n","train_dataset = CustomDataset(train_image_path, train_label_path, train_bboxes_path, train=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","valid_dataset = CustomDataset(valid_image_path, valid_label_path, valid_bboxes_path, train=False)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"HC-Y7FjKxje6","outputId":"359e628f-0b32-4283-8c54-6d993ad64a8e","executionInfo":{"status":"ok","timestamp":1699506832125,"user_tz":420,"elapsed":133,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndataiter = enumerate(train_loader)\\nbatch_idx, (image, var) = next(dataiter)\\n\\nprint(f\"images shape {image.shape}\")\\nprint(f\"labels shape {var[\\'labels\\'].shape}\")\\nprint(f\"bboxes shape {var[\\'boxes\\'].shape}\")\\n\\nplt.imshow(image[0].permute(1,2,0).data)\\nplt.show()\\n\\n# plt.imshow(img_array, cmap=\\'gray\\')\\n# plt.show()\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["import matplotlib.pyplot as plt\n","\n","\"\"\"\n","dataiter = enumerate(train_loader)\n","batch_idx, (image, var) = next(dataiter)\n","\n","print(f\"images shape {image.shape}\")\n","print(f\"labels shape {var['labels'].shape}\")\n","print(f\"bboxes shape {var['boxes'].shape}\")\n","\n","plt.imshow(image[0].permute(1,2,0).data)\n","plt.show()\n","\n","# plt.imshow(img_array, cmap='gray')\n","# plt.show()\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"84T6_xQqzt39"},"source":["load model"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RP3nAS-rzvC2","executionInfo":{"status":"ok","timestamp":1699506833460,"user_tz":420,"elapsed":192,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}}},"outputs":[],"source":["import torchvision\n","import torch.nn.functional as F\n","import torch\n","\n","from torch import nn\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","class TwoMLPHead(nn.Module):\n","    \"\"\"\n","    Standard heads for FPN-based models\n","\n","    Args:\n","        in_channels (int): number of input channels\n","        representation_size (int): size of the intermediate representation\n","    \"\"\"\n","\n","    def __init__(self, in_channels, representation_size):\n","        super().__init__()\n","\n","        self.fc6 = nn.Linear(in_channels, representation_size)\n","        self.fc7 = nn.Linear(representation_size, representation_size)\n","\n","    def forward(self, x):\n","        x = x.flatten(start_dim=1)\n","\n","        x = F.relu(self.fc6(x))\n","        x = F.relu(self.fc7(x))\n","\n","        return x\n","\n","class FastRCNNPredictor(nn.Module):\n","    \"\"\"\n","    Standard classification + bounding box regression layers\n","    for Fast R-CNN.\n","\n","    Args:\n","        in_channels (int): number of input channels\n","        num_classes (int): number of output classes (including background)\n","    \"\"\"\n","\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.cls_score = nn.Linear(in_channels, num_classes)\n","        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n","\n","    def forward(self, x):\n","        if x.dim() == 4:\n","            torch._assert(\n","                list(x.shape[2:]) == [1, 1],\n","                f\"x has the wrong shape, expecting the last two dimensions to be [1,1] instead of {list(x.shape[2:])}\",\n","            )\n","        x = x.flatten(start_dim=1)\n","        scores = self.cls_score(x)\n","        bbox_deltas = self.bbox_pred(x)\n","\n","        return scores, bbox_deltas\n","\n","# A Nano backbone.\n","class NanoBackbone(nn.Module):\n","    def __init__(self, initialize_weights=True, num_classes=1000):\n","        super(NanoBackbone, self).__init__()\n","\n","        self.num_classes = num_classes\n","        self.features = self._create_conv_layers()\n","\n","        if initialize_weights:\n","            # Random initialization of the weights\n","            # just like the original paper.\n","            self._initialize_weights()\n","\n","    def _create_conv_layers(self):\n","        conv_layers = nn.Sequential(\n","            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(64, 128, 3, padding=1),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(128, 256, 1),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Conv2d(256, 256, 3, padding=1),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Conv2d(256, 256, 1),\n","            nn.LeakyReLU(0.1, inplace=True),\n","        )\n","        return conv_layers\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal(m.weight, mode='fan_in',\n","                    nonlinearity='leaky_relu'\n","                )\n","                if m.bias is not None:\n","                        nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","def create_model(num_classes, pretrained=True, coco_model=False):\n","    # Load the backbone features.\n","    backbone = NanoBackbone(num_classes=11).features\n","\n","    # We need the output channels of the last convolutional layers from\n","    # the features for the Faster RCNN model.\n","    backbone.out_channels = 256\n","\n","    # Generate anchors using the RPN. Here, we are using 5x3 anchors.\n","    # Meaning, anchors with 5 different sizes and 3 different aspect\n","    # ratios.\n","    anchor_generator = AnchorGenerator(\n","        sizes=((32, 64, 128, 256, 512),),\n","        aspect_ratios=((0.5, 1.0, 2.0),)\n","    )\n","\n","    # Feature maps to perform RoI cropping.\n","    # If backbone returns a Tensor, `featmap_names` is expected to\n","    # be [0]. We can choose which feature maps to use.\n","    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","        featmap_names=['0'],\n","        output_size=7,\n","        sampling_ratio=2\n","    )\n","\n","    representation_size = 128\n","\n","    # Box head.\n","    box_head = TwoMLPHead(\n","        in_channels=backbone.out_channels * roi_pooler.output_size[0] ** 2,\n","        representation_size=representation_size\n","    )\n","\n","    # Box predictor.\n","    box_predictor = FastRCNNPredictor(representation_size, num_classes)\n","\n","    # Final Faster RCNN model.\n","    model = FasterRCNN(\n","        backbone=backbone,\n","        num_classes=None, # Num classes shoule be None when `box_predictor` is provided.\n","        rpn_anchor_generator=anchor_generator,\n","        box_roi_pool=roi_pooler,\n","        box_head=box_head,\n","        box_predictor=box_predictor\n","    )\n","    return model"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8g-GIViQz-Fa","outputId":"7c22d691-2af9-4d90-c8bf-d4fe70162497","executionInfo":{"status":"ok","timestamp":1699510095722,"user_tz":420,"elapsed":3219417,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-cd2c3baa2a5b>:94: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","  nn.init.kaiming_normal(m.weight, mode='fan_in',\n","100%|██████████| 1100/1100 [27:05<00:00,  1.48s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0 loss 973.9763357937336\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1100/1100 [26:30<00:00,  1.45s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 loss 508.4716504216194\n"]}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model = create_model(num_classes=11, pretrained=True, coco_model=False)\n","model.to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","num_epochs = 2\n","batchsize = 50\n","MODEL_PATH = 'object_detection_rcnn.pth'\n","\"\"\"\n","checkpoint = torch.load(MODEL_PATH)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optim_state_dict'])\n","\"\"\"\n","\n","train_dataset = CustomDataset(train_image_path, train_label_path, train_bboxes_path, train=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n","\n","valid_dataset = CustomDataset(valid_image_path, valid_label_path, valid_bboxes_path, train=False)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n","\n","for eprochs in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    train_loader = tqdm(train_loader)\n","    for batch_idx, data in enumerate(train_loader):\n","        # print(batch_idx)\n","        images = []\n","        targets = []\n","        # print(data)\n","        for i in range(batchsize):\n","\n","            images.append(data[\"image\"][i].to(device))\n","            target = {}\n","            target['boxes'] = data['boxes'][i].to(device)\n","            target['labels'] = data['labels'][i].to(device)\n","            targets.append(target)\n","            # print(data['labels'][i])\n","\n","        loss_dict = model(images, targets)\n","        loss = sum(loss for loss in loss_dict.values())\n","        epoch_loss += loss.cpu().detach().numpy()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    print(f\"epoch {eprochs} loss {epoch_loss}\")\n","    torch.save({'model_state_dict': model.state_dict(),\n","        'optim_state_dict': optimizer.state_dict()}, MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttgW9w2aC9Gu","executionInfo":{"status":"aborted","timestamp":1699506842115,"user_tz":420,"elapsed":4,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}}},"outputs":[],"source":["# testing model\n","dataiter = enumerate(valid_loader)\n","batch_idx, (var) = next(dataiter)\n","\n","print(f\"images shape {var['image'].shape}\")\n","print(f\"labels shape {var['labels'].shape}\")\n","print(f\"bboxes shape {var['boxes'].shape}\")\n","\n","plt.imshow(var['image'][0].permute(1,2,0).data)\n","plt.show()\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model = create_model(num_classes=81, pretrained=True, coco_model=False)\n","model.eval()\n","MODEL_PATH = 'object_detection_rcnn.pth'\n","checkpoint = torch.load(MODEL_PATH)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","image = [var['image'][0]]\n","# print(f\"images shape {image.shape}\")\n","output = model(image)\n","print(output[\"boxes\"])\n","# plt.imshow(img_array, cmap='gray')\n","# plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}