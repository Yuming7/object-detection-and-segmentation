{"cells":[{"cell_type":"markdown","metadata":{"id":"9A_Q39dIxNY5"},"source":["This is the second attempt, gunna do yolo for object detection and gunna use unet for semantic segmentaion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EEM0Zn-xJbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699504723895,"user_tz":420,"elapsed":17203,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}},"outputId":"d3b83d98-1f8f-474f-f119-75732acc8faa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# Google Colab Patch\n","use_colab = True\n","if use_colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import sys\n","    # ----------------------------------------\n","    dir = \"/content/drive/MyDrive/Colab Notebooks/328/assignment4\"\n","    # ----------------------------------------\n","    sys.path.append(dir)\n","from A4_utils import *"]},{"cell_type":"markdown","metadata":{"id":"r1UQY1iQGUzX"},"source":["laod in dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGt5fkizGSlE"},"outputs":[],"source":["import torch\n","from torch.utils.data.dataset import Dataset  # For custom data-sets\n","from torchvision import transforms\n","import torchvision\n","from skimage.io import imread\n","from PIL import Image\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from tqdm import tqdm\n","\n","CLASSES= [\n","    '__background__', '0'\n","    '1', '2', '3',\n","    '4', '5', '6',\n","    '7', '8', '9'\n","]\n","\n","Num_classes = 11\n","\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/328/assignment4/mnistdd_rgb_train_valid/'\n","\n","# training\n","train_image_path = data_path + 'train_X.npy'\n","train_label_path = data_path + 'train_Y.npy'\n","train_bboxes_path = data_path + 'train_bboxes.npy'\n","train_seg_path = data_path + 'train_seg.npy'\n","\n","# validation\n","valid_image_path= data_path + 'valid_X.npy'\n","valid_label_path= data_path +'valid_Y.npy'\n","valid_bboxes_path = data_path +'valid_bboxes.npy'\n","valid_seg_path = data_path +'valid_seg.npy'\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, label_paths, masks_paths, train=True):   # initial logic happens like transform\n","        self.images = np.load(image_paths)\n","        self.label =  np.load(label_paths)\n","        self.mask = np.load(masks_paths)\n","        self.transforms_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n","                                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","    def __getitem__(self, index):\n","\n","         # Retrieve and preprocess a sample\n","        image = self.images[index].reshape(64, 64, 3)\n","        image = np.array(image, dtype=np.float32)\n","        image = image /255\n","        label = self.label[index]\n","        mask = self.mask[index].reshape(64, 64)\n","\n","        image = self.transforms_image(image)\n","        mask = torch.from_numpy(mask).type(torch.LongTensor)\n","        return image, mask\n","\n","    def __len__(self):  # return count of sample we have\n","        return len(self.images)"]},{"cell_type":"markdown","metadata":{"id":"88Uz_a2gHpka"},"source":["Define the unet model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvYWFtIyHoz7","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1699504728113,"user_tz":420,"elapsed":9,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}},"outputId":"e80e025d-f183-40bd-fc9b-9c01dabb68ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nclass Decoder(nn.Module):\\n    def __init__(self, in_channel, mid_channel, out_channel):\\n        super(Decoder, self).__init__()\\n\\n        self.conv = nn.Conv2d(in_channel, mid_channel, kernel_size=3, stride=1, padding=1) #keep ratio\\n        self.conv_trans = nn.ConvTranspose2d(mid_channel, out_channel, kernel_size=4, stride=2, padding=1)\\n\\n    def forward(self, x):\\n        x = F.relu(self.conv(x), inplace=True)\\n        x = F.relu(self.conv_trans(x), inplace=True)\\n        return x\\n\\nclass Unet_resnet18(nn.Module):\\n    def __init__(self, n_classes):\\n        super(Unet_resnet18, self).__init__()\\n\\n        #encoder\\n        self.encoder = models.resnet18(pretrained=True)\\n\\n        self.pool = nn.MaxPool2d(2, 2)\\n        self.conv1 = nn.Sequential(self.encoder.conv1, self.encoder.bn1,\\n                                  self.encoder.relu, self.pool) #64\\n        self.conv2 = self.encoder.layer1 #64\\n        self.conv3 = self.encoder.layer2 #128\\n        self.conv4 = self.encoder.layer3 #256\\n        self.conv5 = self.encoder.layer4 #depth 512\\n\\n        #center\\n        self.center = Decoder(512, 312, 256)\\n\\n        #decoder\\n        self.decoder5 = Decoder(256+512, 256, 256)\\n        self.decoder4 = Decoder(256+256, 128, 128)\\n        self.decoder3 = Decoder(128+128, 64, 64)\\n        self.decoder2 = Decoder(64+64, 32, 32)\\n        self.decoder1 = Decoder(32, 16, 16)\\n        self.decoder0 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\\n\\n        self.final = nn.Conv2d(8, n_classes, kernel_size=1)\\n\\n    def forward(self, x):\\n\\n        #encoder\\n        conv1 = self.conv1(x) #64x64\\n        conv2 = self.conv2(conv1) #32x32\\n        conv3 = self.conv3(conv2) #16x16\\n        conv4 = self.conv4(conv3) #8x8\\n        conv5 = self.conv5(conv4) #4x4\\n\\n        center = self.center(self.pool(conv5)) #4x4\\n        #decoder\\n        dec5 = self.decoder5(torch.cat([center, conv5], 1)) #8x8\\n        dec4 = self.decoder4(torch.cat([dec5, conv4], 1)) #16x16\\n        dec3 = self.decoder3(torch.cat([dec4, conv3], 1)) #32x32\\n        dec2 = self.decoder2(torch.cat([dec3, conv2], 1)) #64x64\\n        dec1 = self.decoder1(dec2) #128x128\\n        dec0 = F.relu(self.decoder0(dec1))\\n\\n        final = torch.sigmoid(self.final(dec0))\\n\\n        return final\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["\"\"\"\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNET(nn.Module):\n","    def __init__(\n","            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n","    ):\n","        super(UNET, self).__init__()\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Down part of UNET\n","        for feature in features:\n","            self.downs.append(DoubleConv(in_channels, feature))\n","            in_channels = feature\n","\n","        # Up part of UNET\n","        for feature in reversed(features):\n","            self.ups.append(\n","                nn.ConvTranspose2d(\n","                    feature*2, feature, kernel_size=2, stride=2,\n","                )\n","            )\n","            self.ups.append(DoubleConv(feature*2, feature))\n","\n","        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []\n","\n","        for down in self.downs:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)\n","        skip_connections = skip_connections[::-1]\n","\n","        for idx in range(0, len(self.ups), 2):\n","            x = self.ups[idx](x)\n","            skip_connection = skip_connections[idx//2]\n","\n","            if x.shape != skip_connection.shape:\n","                x = TF.resize(x, size=skip_connection.shape[2:])\n","\n","            concat_skip = torch.cat((skip_connection, x), dim=1)\n","            x = self.ups[idx+1](concat_skip)\n","\n","        return self.final_conv(x)\n","\"\"\"\n","\n","\n","class UNET(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n","        self.conv2 = self.contract_block(32, 64, 3, 1)\n","        self.conv3 = self.contract_block(64, 128, 3, 1)\n","\n","        self.upconv3 = self.expand_block(128, 64, 3, 1)\n","        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n","        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n","\n","    def __call__(self, x):\n","\n","        # downsampling part\n","        conv1 = self.conv1(x)\n","        conv2 = self.conv2(conv1)\n","        conv3 = self.conv3(conv2)\n","\n","        upconv3 = self.upconv3(conv3)\n","\n","        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n","        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n","\n","        return upconv1\n","\n","    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n","\n","        contract = nn.Sequential(\n","            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n","            torch.nn.BatchNorm2d(out_channels),\n","            torch.nn.ReLU(),\n","            #Sine(),\n","            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n","            torch.nn.BatchNorm2d(out_channels),\n","            torch.nn.ReLU(),\n","            #Sine(),\n","            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","                                 )\n","        return contract\n","\n","    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n","\n","        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n","                            torch.nn.BatchNorm2d(out_channels),\n","                            torch.nn.ReLU(),\n","                            #Sine(),\n","                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n","                            torch.nn.BatchNorm2d(out_channels),\n","                            torch.nn.ReLU(),\n","                            #Sine(),\n","                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n","                            )\n","        return expand\n","\n","\"\"\"\n","class Decoder(nn.Module):\n","    def __init__(self, in_channel, mid_channel, out_channel):\n","        super(Decoder, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_channel, mid_channel, kernel_size=3, stride=1, padding=1) #keep ratio\n","        self.conv_trans = nn.ConvTranspose2d(mid_channel, out_channel, kernel_size=4, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv(x), inplace=True)\n","        x = F.relu(self.conv_trans(x), inplace=True)\n","        return x\n","\n","class Unet_resnet18(nn.Module):\n","    def __init__(self, n_classes):\n","        super(Unet_resnet18, self).__init__()\n","\n","        #encoder\n","        self.encoder = models.resnet18(pretrained=True)\n","\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv1 = nn.Sequential(self.encoder.conv1, self.encoder.bn1,\n","                                  self.encoder.relu, self.pool) #64\n","        self.conv2 = self.encoder.layer1 #64\n","        self.conv3 = self.encoder.layer2 #128\n","        self.conv4 = self.encoder.layer3 #256\n","        self.conv5 = self.encoder.layer4 #depth 512\n","\n","        #center\n","        self.center = Decoder(512, 312, 256)\n","\n","        #decoder\n","        self.decoder5 = Decoder(256+512, 256, 256)\n","        self.decoder4 = Decoder(256+256, 128, 128)\n","        self.decoder3 = Decoder(128+128, 64, 64)\n","        self.decoder2 = Decoder(64+64, 32, 32)\n","        self.decoder1 = Decoder(32, 16, 16)\n","        self.decoder0 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\n","\n","        self.final = nn.Conv2d(8, n_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","\n","        #encoder\n","        conv1 = self.conv1(x) #64x64\n","        conv2 = self.conv2(conv1) #32x32\n","        conv3 = self.conv3(conv2) #16x16\n","        conv4 = self.conv4(conv3) #8x8\n","        conv5 = self.conv5(conv4) #4x4\n","\n","        center = self.center(self.pool(conv5)) #4x4\n","        #decoder\n","        dec5 = self.decoder5(torch.cat([center, conv5], 1)) #8x8\n","        dec4 = self.decoder4(torch.cat([dec5, conv4], 1)) #16x16\n","        dec3 = self.decoder3(torch.cat([dec4, conv3], 1)) #32x32\n","        dec2 = self.decoder2(torch.cat([dec3, conv2], 1)) #64x64\n","        dec1 = self.decoder1(dec2) #128x128\n","        dec0 = F.relu(self.decoder0(dec1))\n","\n","        final = torch.sigmoid(self.final(dec0))\n","\n","        return final\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kxKMXfAH8nq"},"outputs":[],"source":["def check_accuracy(loader, model, device=\"cuda\"):\n","\n","    num_correct = 0\n","    num_pixels = 0\n","    dice_score = 0\n","    images = np.load(data_path + \"valid_X.npy\")\n","    gt_truth = np.load(valid_seg_path)\n","    model.eval()\n","\n","    with torch.no_grad():\n","      N = images.shape[0]\n","      pred_seg = np.empty((N, 4096), dtype=np.int32)\n","      device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","      transforms_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n","                                              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","      images = np.array(images, dtype=np.float32)\n","      images = torch.from_numpy(images)\n","      for i in range(N):\n","          # print(images[i].shape)\n","          image = images[i].reshape(64, 64, 3)\n","          image = np.array(image, dtype=np.float32)\n","          image = transforms_image(image)\n","          image = image.reshape(1, 3, 64, 64)\n","          image = image.to(device)\n","          pred_seg[i] = model(image).argmax(dim=1).flatten().cpu()\n","\n","    acc = compute_segmentation_acc(pred_seg, gt_truth)\n","    print(\n","        f\"Got with acc {acc*100 :.2f}\"\n","    )\n","    model.train()\n","\n","def train_function(data, model, optimizer, loss_fn, device):\n","    print('Entering into train function')\n","    loss_values = []\n","    data = tqdm(data)\n","    for index, batch in enumerate(data):\n","        X, y = batch\n","        X, y = X.to(device), y.to(device)\n","        preds = model(X)\n","\n","        loss = loss_fn(preds, y)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    return loss.item()\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","\n","def compute_classification_acc(pred, gt):\n","    assert pred.shape == gt.shape\n","    return (pred == gt).astype(int).sum() / gt.size\n","\n","\n","def compute_segmentation_acc(pred, gt):\n","    # pred value should be from 0 to 10, where 10 is the background.\n","    assert pred.shape == gt.shape\n","\n","    return (pred == gt).astype(int).sum() / gt.size"]},{"cell_type":"markdown","metadata":{"id":"nGcBr9a5OgM1"},"source":["check to see some smaples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TsQ1XGOOujJ"},"outputs":[],"source":["train_dataset = CustomDataset(train_image_path, train_label_path, train_seg_path, train=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","valid_dataset = CustomDataset(valid_image_path, valid_label_path, valid_seg_path, train=False)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qK23AoUoOftB","colab":{"base_uri":"https://localhost:8080/","height":902},"executionInfo":{"status":"ok","timestamp":1699504741536,"user_tz":420,"elapsed":270,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}},"outputId":"734317ad-345b-4286-dae8-60dad46f8061"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]},{"output_type":"stream","name":"stdout","text":["images shape torch.Size([16, 3, 64, 64])\n","segmentation_masks shape torch.Size([16, 64, 64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","\n","dataiter = enumerate(train_loader)\n","batch_idx, (images, target) = next(dataiter)\n","\n","print(f\"images shape {images.shape}\")\n","# print(f\"labels shape {target['labels'].shape}\")\n","print(f\"segmentation_masks shape {target.shape}\")\n","\n","plt.imshow(images[0].permute(1,2,0).data)\n","plt.show()\n","plt.imshow(target[0].data)\n","plt.show()\n","\n","# plt.imshow(img_array, cmap='gray')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"a61_c7jLLqpA","executionInfo":{"status":"error","timestamp":1699505798620,"user_tz":420,"elapsed":1057086,"user":{"displayName":"Yuming Zhang","userId":"16885747431569181523"}},"outputId":"0d83a30f-22aa-4bb6-8975-915a8840a4e5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 130MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Got with acc 23.50\n","Epoch: 0\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:43<00:00, 12.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.18367671966552734\n","Got with acc 94.19\n","Epoch completed and model successfully saved!\n","Epoch: 1\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:43<00:00, 12.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.16669391095638275\n","Got with acc 93.53\n","Epoch completed and model successfully saved!\n","Epoch: 2\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:43<00:00, 12.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.15431812405586243\n","Got with acc 93.26\n","Epoch completed and model successfully saved!\n","Epoch: 3\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:43<00:00, 12.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.10956863313913345\n","Got with acc 90.48\n","Epoch completed and model successfully saved!\n","Epoch: 4\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:43<00:00, 12.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.06745804101228714\n","Got with acc 92.90\n","Epoch completed and model successfully saved!\n","Epoch: 5\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:43<00:00, 12.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.024807685986161232\n","Got with acc 72.73\n","Epoch completed and model successfully saved!\n","Epoch: 6\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:43<00:00, 12.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.026005558669567108\n","Got with acc 73.56\n","Epoch completed and model successfully saved!\n","Epoch: 7\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.025813942775130272\n","Got with acc 77.12\n","Epoch completed and model successfully saved!\n","Epoch: 8\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.025479966774582863\n","Got with acc 81.38\n","Epoch completed and model successfully saved!\n","Epoch: 9\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.019607778638601303\n","Got with acc 70.67\n","Epoch completed and model successfully saved!\n","Epoch: 10\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.018883787095546722\n","Got with acc 75.84\n","Epoch completed and model successfully saved!\n","Epoch: 11\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.01921348087489605\n","Got with acc 87.75\n","Epoch completed and model successfully saved!\n","Epoch: 12\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.0206513162702322\n","Got with acc 90.65\n","Epoch completed and model successfully saved!\n","Epoch: 13\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.016881898045539856\n","Got with acc 78.20\n","Epoch completed and model successfully saved!\n","Epoch: 14\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.015909286215901375\n","Got with acc 79.53\n","Epoch completed and model successfully saved!\n","Epoch: 15\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.017762182280421257\n","Got with acc 43.81\n","Epoch completed and model successfully saved!\n","Epoch: 16\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.015655498951673508\n","Got with acc 71.92\n","Epoch completed and model successfully saved!\n","Epoch: 17\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.015382515266537666\n","Got with acc 48.55\n","Epoch completed and model successfully saved!\n","Epoch: 18\n","Entering into train function\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 550/550 [00:42<00:00, 12.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.014518490992486477\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-0746f292b8bc>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;34m'loss_values'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLOSS_VALS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     }, MODEL_PATH)\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch completed and model successfully saved!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-dff91c92c883>\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(loader, model, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mpred_seg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_segmentation_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-2c7fe961d2bd>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mconv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mupconv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mupconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupconv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torchvision.models as models\n","# Hyperparameters etc.\n","LEARNING_RATE = 1e-3\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 100\n","NUM_EPOCHS = 20\n","weight_decay = 1e-4\n","\n","PIN_MEMORY = True\n","MODEL_PATH = 'model.pth'\n","LOSS_VALS = []\n","\n","backbone_model = models.resnet18(pretrained=True)\n","\n","train_dataset = CustomDataset(train_image_path, train_label_path, train_seg_path, train=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","valid_dataset = CustomDataset(valid_image_path, valid_label_path, valid_seg_path, train=False)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","\n","model = UNET(3, 11).to(DEVICE).train()\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay = weight_decay)\n","\n","\n","check_accuracy(valid_loader, model, device=DEVICE)\n","\n","#Training the model for every epoch.\n","for e in range(NUM_EPOCHS):\n","    print(f'Epoch: {e}')\n","    loss_val = train_function(train_loader, model, optimizer, loss_fn, DEVICE)\n","    print(loss_val)\n","    LOSS_VALS.append(loss_val)\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'optim_state_dict': optimizer.state_dict(),\n","        'epoch': e,\n","        'loss_values': LOSS_VALS\n","    }, MODEL_PATH)\n","    check_accuracy(valid_loader, model, device=DEVICE)\n","    print(\"Epoch completed and model successfully saved!\")"]},{"cell_type":"code","source":["!python3 \"gdrive/MyDrive/Colab Notebooks/328/assignment4/A4_main.py\""],"metadata":{"id":"NK_bsZh8K_7H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmOhr73nJZ51"},"source":["testing with some output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZbuUS2tJZUL"},"outputs":[],"source":["images = np.load(data_path + \"valid_X.npy\")\n","gt_truth = np.load(valid_seg_path)\n","\n","N = images.shape[0]\n","pred_seg = np.empty((N, 4096), dtype=np.int32)\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","transforms_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n","                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","images = np.array(images, dtype=np.float32)\n","images = torch.from_numpy(images)\n","for i in range(N):\n","    # print(images[i].shape)\n","    image = images[i].reshape(64, 64, 3)\n","    image = np.array(image, dtype=np.float32)\n","    image = transforms_image(image)\n","    image = image.reshape(1, 3, 64, 64)\n","    image = image.to(device)\n","    pred_seg[i] = model(image).argmax(dim=1).flatten().cpu()\n","\n","acc = compute_segmentation_acc(pred_seg, gt_truth)\n","\n","print(f\"accury = {acc}\")\n","image = images[0].reshape(64, 64, 3)\n","image = np.array(image, dtype=np.float32)\n","image = transforms_image(image)\n","\n","predecited_mask = torch.from_numpy(pred_seg[0])\n","predecited_mask = predecited_mask.reshape(64, 64)\n","# plt.imshow(image.permute(1,2,0).data)\n","# plt.show()\n","print(predecited_mask.data)\n","plt.imshow(predecited_mask.data)\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CKsMLtztJQYi"},"source":["A4_main.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TzCFrNOvomL"},"outputs":[],"source":["import numpy as np\n","import torch\n","import os\n","import shutil\n","\n","\n","def detect_and_segment(images):\n","    \"\"\"\n","\n","    :param np.ndarray images: N x 12288 array containing N 64x64x3 images flattened into vectors\n","    :return: np.ndarray, np.ndarray\n","    \"\"\"\n","    N = images.shape[0]\n","\n","    # pred_class: Your predicted labels for the 2 digits, shape [N, 2]\n","    pred_class = np.empty((N, 2), dtype=np.int32)\n","    # pred_bboxes: Your predicted bboxes for 2 digits, shape [N, 2, 4]\n","    pred_bboxes = np.empty((N, 2, 4), dtype=np.float64)\n","    # pred_seg: Your predicted segmentation for the image, shape [N, 4096]\n","    pred_seg = np.empty((N, 4096), dtype=np.int32)\n","\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # pred_seg: Your predicted segmentation for the image, shape [N, 4096]\n","    pred_seg = np.empty((N, 4096), dtype=np.int32)\n","\n","    model = UNET(3, 11).to(device)\n","    checkpoint = torch.load(\"saved_model.pth\")\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","\n","    transforms_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n","                                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","\n","    images = np.array(images, dtype=np.float32)\n","    images = torch.from_numpy(images)\n","\n","    for i in range(N):\n","        # print(images[i].shape)\n","        image = images[i].reshape(64, 64, 3)\n","        image = np.array(image, dtype=np.float32)\n","        image = transforms_image(image)\n","        image = image.reshape(1, 3, 64, 64)\n","        image = image.to(device)\n","        pred_seg[i] = model(image).argmax(dim=1).flatten().cpu()\n","\n","\n","    # add your code here to fill in pred_class and pred_bboxes\n","\n","    return pred_class, pred_bboxes, pred_seg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ci1wVRyKvppB"},"outputs":[],"source":["import os.path\n","import timeit\n","import numpy as np\n","from skimage.draw import polygon\n","\n","\n","def compute_classification_acc(pred, gt):\n","    assert pred.shape == gt.shape\n","    return (pred == gt).astype(int).sum() / gt.size\n","\n","\n","def compute_segmentation_acc(pred, gt):\n","    # pred value should be from 0 to 10, where 10 is the background.\n","    assert pred.shape == gt.shape\n","\n","    return (pred == gt).astype(int).sum() / gt.size\n","\n","\n","def get_iou(bbox_pred, bbox_gt, L_pred, L_gt):\n","    \"\"\"all pixel coordinates within the prediction bounding box\"\"\"\n","    rr, cc = polygon([bbox_pred[0], bbox_pred[0], bbox_pred[2], bbox_pred[2]],\n","                     [bbox_pred[1], bbox_pred[3], bbox_pred[3], bbox_pred[1]], [64, 64])\n","    L_pred[rr, cc] = 1\n","\n","    \"\"\"all pixel coordinates within the GT bounding box\"\"\"\n","    rr, cc = polygon([bbox_gt[0], bbox_gt[0], bbox_gt[2], bbox_gt[2]],\n","                     [bbox_gt[1], bbox_gt[3], bbox_gt[3], bbox_gt[1]], [64, 64])\n","    L_gt[rr, cc] = 1\n","\n","    L_sum = L_pred + L_gt\n","    intersection = np.sum(L_sum == 2)\n","    union = np.sum(L_sum >= 1)\n","\n","    iou = intersection / union\n","\n","    L_pred[:, :] = 0\n","    L_gt[:, :] = 0\n","\n","    return iou\n","\n","\n","def compute_mean_iou(bboxes_pred, bboxes_gt, classes_pred, classes_gt):\n","    \"\"\"\n","\n","    :param bboxes_pred: predicted bounding boxes, shape=(n_images,2,4)\n","    :param bboxes_gt: ground truth bounding boxes, shape=(n_images,2,4)\n","    :param classes_pred: predicted classes, shape=(n_images,2)\n","    :param classes_gt: ground truth classes, shape=(n_images,2)\n","    :return:\n","    \"\"\"\n","\n","    n_images = np.shape(bboxes_gt)[0]\n","    L_pred = np.zeros((64, 64))\n","    L_gt = np.zeros((64, 64))\n","    iou_sum = 0.0\n","    for i in range(n_images):\n","        iou1 = get_iou(bboxes_pred[i, 0, :], bboxes_gt[i, 0, :], L_pred, L_gt)\n","        iou2 = get_iou(bboxes_pred[i, 1, :], bboxes_gt[i, 1, :], L_pred, L_gt)\n","\n","        iou_sum1 = iou1 + iou2\n","\n","        if classes_pred[i, 0] == classes_pred[i, 1] and classes_gt[i, 0] == classes_gt[i, 1]:\n","            iou1 = get_iou(bboxes_pred[i, 0, :], bboxes_gt[i, 1, :], L_pred, L_gt)\n","            iou2 = get_iou(bboxes_pred[i, 1, :], bboxes_gt[i, 0, :], L_pred, L_gt)\n","\n","            iou_sum2 = iou1 + iou2\n","\n","            if iou_sum2 > iou_sum1:\n","                iou_sum1 = iou_sum2\n","\n","        iou_sum += iou_sum1\n","\n","    mean_iou = iou_sum / (2. * n_images)\n","\n","    return mean_iou\n","\n","\n","class Params:\n","    def __init__(self):\n","        # self.prefix = \"test\"\n","        self.prefix = \"valid\"\n","        # self.prefix = \"train\"\n","        self.load = 1\n","        self.save = 1\n","        self.load_path = 'saved_preds.npz'\n","        self.vis = 0\n","        self.vis_size = (300, 300)\n","        self.show_det = 0\n","        self.show_seg = 1\n","\n","        self.speed_thresh = 10\n","        self.acc_thresh = (0.7, 0.98)\n","        self.iou_thresh = (0.7, 0.98)\n","        self.seg_thresh = (0.7, 0.98)\n","\n","        self.class_cols = {\n","            0: 'red',\n","            1: 'green',\n","            2: 'blue',\n","            3: 'magenta',\n","            4: 'cyan',\n","            5: 'yellow',\n","            6: 'purple',\n","            7: 'forest_green',\n","            8: 'orange',\n","            9: 'white',\n","            10: 'black',\n","        }\n","\n","\n","def compute_score(res, thresh):\n","    min_thres, max_thres = thresh\n","\n","    if res < min_thres:\n","        score = 0.0\n","    elif res > max_thres:\n","        score = 100.0\n","    else:\n","        score = float(res - min_thres) / (max_thres - min_thres) * 100\n","    return score\n","\n","\n","def main():\n","    params = Params()\n","\n","    try:\n","        import paramparse\n","    except ImportError:\n","        pass\n","    else:\n","        paramparse.process(params)\n","\n","    prefix = params.prefix\n","\n","    images = np.load(data_path+prefix + \"_X.npy\")\n","    gt_classes = np.load(data_path+prefix + \"_Y.npy\")\n","    gt_bboxes = np.load(data_path+prefix + \"_bboxes.npy\")\n","    gt_seg = np.load(data_path+prefix + \"_seg.npy\")\n","\n","    n_images = images.shape[0]\n","\n","    if params.load and os.path.exists(params.load_path):\n","        print(f'loading predictions from {params.load_path}')\n","        saved_preds = np.load(params.load_path)\n","        pred_classes = saved_preds['pred_classes']\n","        pred_bboxes = saved_preds['pred_bboxes']\n","        pred_seg = saved_preds['pred_seg']\n","\n","        test_time = test_speed = 0\n","    else:\n","        print(f'running prediction on {n_images} {prefix} images')\n","        start_t = timeit.default_timer()\n","        pred_classes, pred_bboxes, pred_seg = detect_and_segment(images)\n","        end_t = timeit.default_timer()\n","        test_time = end_t - start_t\n","        assert test_time > 0, \"test_time cannot be 0\"\n","        test_speed = float(n_images) / test_time\n","\n","        if params.save:\n","            np.savez_compressed(params.load_path, pred_classes=pred_classes, pred_bboxes=pred_bboxes, pred_seg=pred_seg)\n","\n","    cls_acc = compute_classification_acc(pred_classes, gt_classes)\n","    iou = compute_mean_iou(pred_bboxes, gt_bboxes, pred_classes, gt_classes)\n","    seg_acc = compute_segmentation_acc(pred_seg, gt_seg)\n","\n","    acc_score = compute_score(cls_acc, params.acc_thresh)\n","    iou_score = 0 # compute_score(iou, params.iou_thresh)\n","    seg_score = compute_score(seg_acc, params.seg_thresh)\n","\n","    if test_speed < params.speed_thresh:\n","        overall_score = 0\n","    else:\n","        overall_score = ((iou_score + acc_score) / 2. + seg_score) / 2.\n","\n","    print(f\"Classification Accuracy: {cls_acc*100:.3f} %\")\n","    print(f\"Detection IOU: {iou*100:.3f} %\")\n","    print(f\"Segmentation Accuracy: {seg_acc*100:.3f} %\")\n","\n","    print(f\"Test time: {test_time:.3f} seconds\")\n","    print(f\"Test speed: {test_speed:.3f} images / second\")\n","\n","    print(f\"Classification Score: {acc_score:.3f}\")\n","    print(f\"IOU Score: {iou_score:.3f}\")\n","    print(f\"Segmentation Score: {seg_score:.3f}\")\n","    print(f\"Overall Score: {overall_score:.3f}\")\n","\n","    if not params.vis:\n","        return\n","\n","    import cv2\n","    from A4_utils import vis_bboxes, vis_seg, annotate\n","\n","    print('press spacebar to toggle pause and escape to quit')\n","    pause_after_frame = 1\n","    for img_id in range(n_images):\n","        src_img = images[img_id, ...].squeeze()\n","        src_img = src_img.reshape((64, 64, 3)).astype(np.uint8)\n","\n","        vis_img = np.copy(src_img)\n","\n","        bbox_1 = gt_bboxes[img_id, 0, :].squeeze().astype(np.int32)\n","        bbox_2 = gt_bboxes[img_id, 1, :].squeeze().astype(np.int32)\n","        y1, y2 = gt_classes[img_id, ...].squeeze()\n","        gt_classes[img_id, ...].squeeze()\n","        vis_img = vis_bboxes(vis_img, bbox_1, bbox_2, y1, y2, params.vis_size)\n","        vis_img_seg_gt = vis_seg(src_img, gt_seg, img_id, params.class_cols, params.vis_size)\n","\n","        vis_img_list = [vis_img, vis_img_seg_gt]\n","        vis_img_labels = ['gt det', 'gt seg']\n","\n","        if params.show_det:\n","            vis_img_det = np.copy(src_img)\n","            bbox_1 = pred_bboxes[img_id, 0, :].squeeze().astype(np.int32)\n","            bbox_2 = pred_bboxes[img_id, 1, :].squeeze().astype(np.int32)\n","            y1, y2 = pred_classes[img_id, ...].squeeze()\n","            gt_classes[img_id, ...].squeeze()\n","            vis_img_det = vis_bboxes(vis_img_det, bbox_1, bbox_2, y1, y2, params.vis_size)\n","            vis_img_list.append(vis_img_det)\n","            vis_img_labels.append('pred det')\n","\n","        if params.show_seg:\n","            vis_img_seg = vis_seg(src_img, pred_seg, img_id, params.class_cols, params.vis_size)\n","            vis_img_list.append(vis_img_seg)\n","            vis_img_labels.append('pred seg')\n","\n","        vis_img = annotate(vis_img_list,\n","                           text=f'image {img_id}',\n","                           img_labels=vis_img_labels, grid_size=(1, -1))\n","        cv2.imshow('vis_img', vis_img)\n","\n","        key = cv2.waitKey(1 - pause_after_frame)\n","        if key == 27:\n","            return\n","        elif key == 32:\n","            pause_after_frame = 1 - pause_after_frame\n","\n","\n","if __name__ == '__main__':\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}